# Robots.txt
A website should be using a robots.txt file. When search engine robots crawl a website, they typically first access a site's robots.txt file. Robots.txt tells Googlebot and other crawlers what is and is not allowed to be crawled on a site.

### Best practice
[Search Console Help](https://support.google.com/webmasters/answer/6062608)
* Use no-index for pages, that shouldnâ€™t be crawled (e.g. search result pages)
